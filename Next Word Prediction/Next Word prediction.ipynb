{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lvo0t7XVIkWZ"
   },
   "source": [
    "## Data, model, and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzpUtDMqmA-x"
   },
   "source": [
    "In this example, we train the model on the combined works of William Shakespeare, then use the model to compose a play in the style of *The Great Bard*:\n",
    "\n",
    "<blockquote>\n",
    "Loves that led me no dumbs lack her Berjoy's face with her to-day.  \n",
    "The spirits roar'd; which shames which within his powers  \n",
    "\tWhich tied up remedies lending with occasion,  \n",
    "A loud and Lancaster, stabb'd in me  \n",
    "\tUpon my sword for ever: 'Agripo'er, his days let me free.  \n",
    "\tStop it of that word, be so: at Lear,  \n",
    "\tWhen I did profess the hour-stranger for my life,  \n",
    "\tWhen I did sink to be cried how for aught;  \n",
    "\tSome beds which seeks chaste senses prove burning;  \n",
    "But he perforces seen in her eyes so fast;  \n",
    "And _  \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRQ6Fjra3Ruq"
   },
   "source": [
    "### Download data\n",
    "\n",
    "Download *The Complete Works of William Shakespeare* as a single text file from [Project Gutenberg](https://www.gutenberg.org/). You use snippets from this file as the *training data* for the model. The *target* snippet is offset by one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8sIXh1DEDDd",
    "outputId": "bc2e34cf-80d7-4642-e61b-d8ba055f5a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-05 21:50:53--  http://www.gutenberg.org/files/100/100-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.gutenberg.org/files/100/100-0.txt [following]\n",
      "--2021-07-05 21:50:53--  https://www.gutenberg.org/files/100/100-0.txt\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --show-progress --continue -O /content/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbL6cqCl7hnt"
   },
   "source": [
    "### Build the input dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7nbGKAHi0dx"
   },
   "source": [
    "We just downloaded some text. The following shows the start of the text and a random snippet so we can get a feel for the whole text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJiYai-GjQRk",
    "outputId": "5a10829e-08a1-40b2-e65f-b97f04c389fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere in the United States and\r\n",
      "most other parts of the world at no cost and with almost no restrictions\r\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
      "...\n",
      "Who is that calls so coldly?\n",
      "Her father is Baptista Minola,\n",
      "\n",
      "“Lord” quoth he! That a monster should be such a natural!\n",
      "Where love is throned.\n"
     ]
    }
   ],
   "source": [
    "!head -n5 /content/shakespeare.txt\n",
    "!echo \"...\"\n",
    "!shuf -n5 /content/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "E3V4V-Jxmuv3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import distutils\n",
    "if distutils.version.LooseVersion(tf.__version__) < '2.0':\n",
    "    raise Exception('This notebook is compatible with TensorFlow 2.0 or higher.')\n",
    "\n",
    "SHAKESPEARE_TXT = '/content/shakespeare.txt'\n",
    "\n",
    "def transform(txt):\n",
    "  return np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
    "\n",
    "def input_fn(seq_len=100, batch_size=1024):\n",
    "  \"\"\"Return a dataset of source and target sequences for training.\"\"\"\n",
    "  with tf.io.gfile.GFile(SHAKESPEARE_TXT, 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "  source = tf.constant(transform(txt), dtype=tf.int32)\n",
    "\n",
    "  ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1, drop_remainder=True)\n",
    "\n",
    "  def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "  BUFFER_SIZE = 10000\n",
    "  ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "  return ds.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bbb05dNynDrQ"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "The model is defined as a two-layer, forward-LSTM, the same model should work both on CPU and TPU.\n",
    "\n",
    "Because our vocabulary size is 256, the input dimension to the Embedding layer is 256.\n",
    "\n",
    "When specifying the arguments to the LSTM, it is important to note how the stateful argument is used. When training we will make sure that `stateful=False` because we do want to reset the state of our model between batches, but when sampling (computing predictions) from a trained model, we want `stateful=True` so that the model can retain information across the current batch and generate more interesting text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yLEM-fLJlEEt"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "\n",
    "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
    "  \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
    "  source = tf.keras.Input(\n",
    "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
    "\n",
    "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
    "  lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
    "  lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
    "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
    "  return tf.keras.Model(inputs=[source], outputs=[predicted_char])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzBYDJI0_Tfm"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "First, we need to create a distribution strategy that can use the TPU. In this case it is TPUStrategy. You can create and compile the model inside its scope. Once that is done, future calls to the standard Keras methods `fit`, `evaluate` and `predict` use the TPU.\n",
    "\n",
    "Again note that we train with `stateful=False` because while training, we only care about one batch at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExQ922tfzSGA",
    "outputId": "ad6ffcbe-5d74-41fe-d6ea-7aa0fe8a831c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.14.184.162:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.14.184.162:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n",
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 12s 61ms/step - loss: 3.2130 - sparse_categorical_accuracy: 0.2005\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 1.9353 - sparse_categorical_accuracy: 0.4351\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 1.4921 - sparse_categorical_accuracy: 0.5485\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 1.3523 - sparse_categorical_accuracy: 0.5834\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 1.2894 - sparse_categorical_accuracy: 0.5995\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 1.2469 - sparse_categorical_accuracy: 0.6109\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 1.2201 - sparse_categorical_accuracy: 0.6181\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 1.2055 - sparse_categorical_accuracy: 0.6217\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 1.1899 - sparse_categorical_accuracy: 0.6256\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 1.1768 - sparse_categorical_accuracy: 0.6291\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 1.1630 - sparse_categorical_accuracy: 0.6332\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 1.1479 - sparse_categorical_accuracy: 0.6373\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 1.1438 - sparse_categorical_accuracy: 0.6382\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 1.1380 - sparse_categorical_accuracy: 0.6398\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 1.1314 - sparse_categorical_accuracy: 0.6413\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 1.1247 - sparse_categorical_accuracy: 0.6433\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 1.1144 - sparse_categorical_accuracy: 0.6464\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 1.1091 - sparse_categorical_accuracy: 0.6478\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 1.1086 - sparse_categorical_accuracy: 0.6478\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 1.1053 - sparse_categorical_accuracy: 0.6484\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "# This is the TPU initialization code that has to be at the beginning.\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "\n",
    "with strategy.scope():\n",
    "  training_model = lstm_model(seq_len=100, stateful=False)\n",
    "  training_model.compile(\n",
    "      optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "training_model.fit(\n",
    "    input_fn(),\n",
    "    steps_per_epoch=100,\n",
    "    epochs=20,verbose=1\n",
    ")\n",
    "training_model.save_weights('/content/bard.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCBtcpZkykSf"
   },
   "source": [
    "### Make predictions with the model\n",
    "\n",
    "Use the trained model to make predictions and generate your own Shakespeare-esque play.\n",
    "Start the model off with a *seed* sentence, then generate 250 characters from it. The model makes five predictions from the initial seed.\n",
    "\n",
    "The predictions are done on the CPU so the batch size (5) in this case does not have to be divisible by 8.\n",
    "\n",
    "Note that when we are doing predictions or, to be more precise, text generation, we set `stateful=True` so that the model's state is kept between batches. If stateful is false, the model state is reset between each batch, and the model will only be able to use the information from the current batch (a single character) to make a prediction.\n",
    "\n",
    "The output of the model is a set of probabilities for the next character (given the input so far). To build a paragraph, we predict one character at a time and sample a character (based on the probabilities provided by the model). For example, if the input character is \"o\" and the output probabilities are \"p\" (0.65), \"t\" (0.30), others characters (0.05), then we allow our model to generate text other than just \"Ophelia\" and \"Othello.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tU7M-EGGxR3E",
    "outputId": "23b7d72d-e53e-4060-f172-3a7cc5e8c883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 0\n",
      "\n",
      "\n",
      " Tis called Love two thunder- But in some conceit\r\n",
      "Reigns take this day a flood of heaven. But, sir, nor haste your puissant\r\n",
      "Upon our queen of your mother;\r\n",
      "My feats full of bitter, and then entertainmen when he came\r\n",
      "Dangers, and run shall ever yea\n",
      "\n",
      "PREDICTION 1\n",
      "\n",
      "\n",
      " he'll no man to be his load-of occasion near;\r\n",
      "Minde hath fight agony with you. What is't, it is not so much more, rise now leave you tell me, I for me.\r\n",
      "Then let him go with the world, be ruld.\r\n",
      "\r\n",
      " [_Exeunt._]\r\n",
      "\r\n",
      "SCENE VII. The same. A room in Cori\n",
      "\n",
      "PREDICTION 2\n",
      "\n",
      "\n",
      " The turk and false rocks\r\n",
      "To let falls upon. Believersy hath tenderd green wit\r\n",
      "Where they may go with gold upon the colour for bounced fast stands\r\n",
      "More rotten years colour are ready out, in him the Duke?\r\n",
      "This is music and head much end on me we d\n",
      "\n",
      "PREDICTION 3\n",
      "\n",
      "\n",
      " the goldsmiths ear\r\n",
      "Than doth offer how many lands of shallow swift streets,\r\n",
      "There to pass.\r\n",
      "\r\n",
      "MACDUFFORD.\r\n",
      "Fortune hagh my mouth where he will.\r\n",
      "\r\n",
      "EDGAR.\r\n",
      "Then take a stranger.\r\n",
      "\r\n",
      "EDGAR.\r\n",
      "[_Aside to Berowne is rumiled swords, pleasing fortunate! T\n",
      "\n",
      "PREDICTION 4\n",
      "\n",
      "\n",
      " Believe me,\r\n",
      "And, give it thee leave to speak leave him not, my hearts ready,\r\n",
      "Yet the meal of Cawdor of th rate than my lips from the king\r\n",
      "From off his condition beard as Troy;\r\n",
      "For I fearther, God be wishes me to your house\r\n",
      "For the very worth of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "PREDICT_LEN = 250\n",
    "\n",
    "# Keras requires the batch size be specified ahead of time for stateful models.\n",
    "# We use a sequence length of 1, as we will be feeding in one character at a \n",
    "# time and predicting the next character.\n",
    "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
    "prediction_model.load_weights('/content/bard.h5')\n",
    "\n",
    "# We seed the model with our initial string, copied BATCH_SIZE times\n",
    "\n",
    "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
    "seed = transform(seed_txt)\n",
    "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "\n",
    "# First, run the seed forward to prime the state of the model.\n",
    "prediction_model.reset_states()\n",
    "for i in range(len(seed_txt) - 1):\n",
    "  prediction_model.predict(seed[:, i:i + 1])\n",
    "\n",
    "# Now we can accumulate predictions!\n",
    "predictions = [seed[:, -1:]]\n",
    "for i in range(PREDICT_LEN):\n",
    "  last_word = predictions[-1]\n",
    "  next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
    "  \n",
    "  # sample from our output distribution\n",
    "  next_idx = [\n",
    "      np.random.choice(256, p=next_probits[i])\n",
    "      for i in range(BATCH_SIZE)\n",
    "  ]\n",
    "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
    "  \n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "  print('PREDICTION %d\\n\\n' % i)\n",
    "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "  generated = ''.join([chr(c) for c in p])  # Convert back to text\n",
    "  print(generated)\n",
    "  print()\n",
    "  assert len(generated) == PREDICT_LEN, 'Generated text too short'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "N6ZDpd9XzFeN"
   ],
   "name": "Copy of Predict Shakespeare with Cloud TPUs and Keras",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
